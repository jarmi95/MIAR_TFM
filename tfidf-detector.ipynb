{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55e0ff01-388a-4740-8c02-40c3004121d7",
   "metadata": {},
   "source": [
    "# Clasificación usando TF-IDF con algoritmos de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06975921-5f39-46a3-ae32-780e7d5cde8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "431159c9-6959-4c07-a696-845aab5af121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Leer el archivo Parquet en un DataFrame\n",
    "df = pd.read_parquet('./eda/mapped_&_usecases_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1182ce7-900d-4f72-9025-277753746785",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of df1: (81096, 7)\n",
      "Size of df2: (34756, 7)\n",
      "Index(['source_code', 'slither_text', 'slither_label', 'use_cases',\n",
      "       'vulnerability_mapping', 'vulnerability_keys', 'vulnerable'],\n",
      "      dtype='object')\n",
      "Tamaño del conjunto de entrenamiento: (12164, 7)\n",
      "Tamaño del conjunto de validación: (12165, 7)\n",
      "Tamaño del conjunto de prueba: (10427, 7)\n",
      "Tamaño del conjunto de entrenamiento sampleado: (12000, 7)\n",
      "Tamaño del conjunto de validación sampleado: (5000, 7)\n",
      "Tamaño del conjunto de prueba sampleado: (500, 7)\n"
     ]
    }
   ],
   "source": [
    "# 3. Dividir los datos en conjunto de entrenamiento y prueba\n",
    "# Establecer una semilla para la reproducibilidad\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Dividir el DataFrame en dos partes iguales\n",
    "df1, df2 = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "# Verificar las dimensiones de las divisiones\n",
    "print(f\"Size of df1: {df1.shape}\")\n",
    "print(f\"Size of df2: {df2.shape}\")\n",
    "\n",
    "print(df2.columns)\n",
    "\n",
    "# Definir el tamaño de las particiones\n",
    "train_size = 0.7  # Porcentaje para el conjunto de entrenamiento\n",
    "val_size = 0.15   # Porcentaje para el conjunto de validación\n",
    "test_size = 0.15  # Porcentaje para el conjunto de prueba\n",
    "\n",
    "# Dividir el DataFrame en entrenamiento + validación y prueba\n",
    "train_val_df, test_df = train_test_split(df2, test_size=test_size + val_size, random_state=42)\n",
    "\n",
    "# Dividir el DataFrame de entrenamiento + validación en entrenamiento y validación\n",
    "val_size_adjusted = val_size / (val_size + test_size)  # Ajuste para la proporción en el conjunto de entrenamiento/validación\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=val_size_adjusted, random_state=42)\n",
    "               \n",
    "# Verificar tamaños de los conjuntos\n",
    "print(f\"Tamaño del conjunto de entrenamiento: {train_df.shape}\")\n",
    "print(f\"Tamaño del conjunto de validación: {val_df.shape}\")\n",
    "print(f\"Tamaño del conjunto de prueba: {test_df.shape}\")\n",
    "\n",
    "# Definir el número de muestras que deseas seleccionar\n",
    "num_train_samples = 12000\n",
    "num_val_samples = 5000\n",
    "num_test_samples = 500\n",
    "\n",
    "# Muestrear X muestras directamente desde los DataFrames\n",
    "train_df_sampled = train_df.sample(n=num_train_samples, replace=False, random_state=42)\n",
    "val_df_sampled = val_df.sample(n=num_val_samples, replace=False, random_state=42)\n",
    "test_df_sampled = test_df.sample(n=num_test_samples, replace=False, random_state=42)\n",
    "\n",
    "# Verificar tamaños de los conjuntos\n",
    "print(f\"Tamaño del conjunto de entrenamiento sampleado: {train_df_sampled.shape}\")\n",
    "print(f\"Tamaño del conjunto de validación sampleado: {val_df_sampled.shape}\")\n",
    "print(f\"Tamaño del conjunto de prueba sampleado: {test_df_sampled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78a6037a-eadd-4892-8e1e-010e366ccac2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Tokenización del código\n",
    "def tokenize_code(code):\n",
    "    # Expresión regular mejorada para capturar tokens específicos de Solidity\n",
    "    tokens = re.findall(r'\\b\\w+\\b|[{}()\\[\\];,=+\\-*/<>!&|%^~]', code)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Aplicar tokenización\n",
    "train_df_sampled['tokenized_code'] = train_df_sampled['source_code'].apply(tokenize_code)\n",
    "test_df_sampled['tokenized_code'] = test_df_sampled['source_code'].apply(tokenize_code)\n",
    "\n",
    "\n",
    "# 2. Separar características (X) y etiquetas (y)\n",
    "X_train_sampled = train_df_sampled['tokenized_code']\n",
    "X_test_sampled = test_df_sampled['tokenized_code']\n",
    "\n",
    "y_train_sampled = train_df_sampled['vulnerable']\n",
    "y_test_sampled = test_df_sampled['vulnerable']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c0346d6-3c5c-408f-9626-bb45fc01d0bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4. Transformación TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_sampled)\n",
    "X_test_tfidf = tfidf.transform(X_test_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07d71e4d-710c-43d5-a5d3-a1ae7fadb87c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: Logistic Regression\n",
      "Infering model: Logistic Regression\n",
      "Training model: Random Forest\n",
      "Infering model: Random Forest\n",
      "Training model: Support Vector Machine\n",
      "Infering model: Support Vector Machine\n",
      "Training model: Gradient Boosting\n",
      "Infering model: Gradient Boosting\n",
      "                    Model  Accuracy  Precision  Recall  F1-Score\n",
      "0     Logistic Regression     0.832   0.826363   0.832  0.825193\n",
      "1           Random Forest     0.869   0.866216   0.869  0.864983\n",
      "2  Support Vector Machine     0.841   0.836299   0.841  0.836582\n",
      "3       Gradient Boosting     0.857   0.853349   0.857  0.852615\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "import os\n",
    "\n",
    "# Crear el directorio de salida si no existe\n",
    "output_dir = './outputs/tfidf/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. Definición de los modelos\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Support Vector Machine\": SVC(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# 2. Entrenamiento, predicción y evaluación de cada modelo\n",
    "results = []\n",
    "confusion_matrices = []\n",
    "classification_reports = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training model: {model_name}\")\n",
    "    \n",
    "    # Entrenamiento\n",
    "    model.fit(X_train_tfidf, y_train_sample)\n",
    "    \n",
    "    print(f\"Infering model: {model_name}\")\n",
    "    # Predicción\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    \n",
    "    # Evaluación de métricas\n",
    "    accuracy = accuracy_score(y_test_sample, y_pred)\n",
    "    precision = precision_score(y_test_sample, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test_sample, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test_sample, y_pred, average='weighted')\n",
    "    \n",
    "    # Agregar resultados al DataFrame\n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1\n",
    "    })\n",
    "\n",
    "    # Generar y guardar la matriz de confusión\n",
    "    cm = confusion_matrix(y_test_sample, y_pred)\n",
    "    confusion_matrices.append({\"Model\": model_name, \"Confusion Matrix\": cm})\n",
    "    \n",
    "    # Generar y almacenar el classification report\n",
    "    report = classification_report(y_test_sample, y_pred)\n",
    "    classification_reports.append({\"Model\": model_name, \"Classification Report\": report})\n",
    "\n",
    "# Convertir resultados a un DataFrame de pandas\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 3. Guardar resultados en un archivo CSV\n",
    "results_df.to_csv(os.path.join(output_dir, 'model_comparison_results_1000.csv'), index=False)\n",
    "\n",
    "# Guardar la matriz de confusión como un archivo CSV\n",
    "confusion_matrices_df = pd.DataFrame(\n",
    "    [(item['Model'], item['Confusion Matrix'].tolist()) for item in confusion_matrices],\n",
    "    columns=['Model', 'Confusion Matrix']\n",
    ")\n",
    "confusion_matrices_df.to_csv(os.path.join(output_dir, 'confusion_matrices_1000.csv'), index=False)\n",
    "\n",
    "# Guardar el classification report en un archivo de texto\n",
    "with open(os.path.join(output_dir, 'classification_reports_1000.txt'), 'w') as file:\n",
    "    for report in classification_reports:\n",
    "        file.write(f\"Model: {report['Model']}\\n\")\n",
    "        file.write(f\"{report['Classification Report']}\\n\")\n",
    "        file.write(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# 4. Mostrar resultados\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddc757c9-6c46-41c2-8e48-48ff5b454cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Grid Search for: Logistic Regression\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/tfmvenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/jupyter/tfmvenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jupyter/tfmvenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jupyter/tfmvenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jupyter/tfmvenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jupyter/tfmvenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jupyter/tfmvenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jupyter/tfmvenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jupyter/tfmvenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jupyter/tfmvenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jupyter/tfmvenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jupyter/tfmvenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jupyter/tfmvenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jupyter/tfmvenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jupyter/tfmvenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/jupyter/tfmvenv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for Logistic Regression: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Infering with best model: Logistic Regression\n",
      "Starting Grid Search for: Random Forest\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best params for Random Forest: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Infering with best model: Random Forest\n",
      "Starting Grid Search for: Support Vector Machine\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best params for Support Vector Machine: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Infering with best model: Support Vector Machine\n",
      "Starting Grid Search for: Gradient Boosting\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "Best params for Gradient Boosting: {'learning_rate': 0.1, 'n_estimators': 200}\n",
      "Infering with best model: Gradient Boosting\n",
      "                    Model                                    Best Parameters  \\\n",
      "0     Logistic Regression  {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}   \n",
      "1           Random Forest  {'max_depth': None, 'min_samples_split': 2, 'n...   \n",
      "2  Support Vector Machine       {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}   \n",
      "3       Gradient Boosting        {'learning_rate': 0.1, 'n_estimators': 200}   \n",
      "\n",
      "   Accuracy  Precision  Recall  F1-Score  \n",
      "0     0.896   0.894661   0.896  0.895055  \n",
      "1     0.912   0.911410   0.912  0.909834  \n",
      "2     0.904   0.904000   0.904  0.904000  \n",
      "3     0.888   0.886060   0.888  0.885243  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import os\n",
    "\n",
    "# Crear el directorio de salida si no existe\n",
    "output_dir = './outputs/detection/tfidf/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. Definición de los modelos y sus hiperparámetros para GridSearchCV\n",
    "model_params = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"model\": LogisticRegression(random_state=42),\n",
    "        \"params\": {\n",
    "            \"penalty\": ['l1', 'l2', 'elasticnet', 'none'],\n",
    "            \"C\": [0.01, 0.1, 1, 10],\n",
    "            \"solver\": ['lbfgs', 'liblinear', 'saga']\n",
    "        }\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"model\": RandomForestClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [50, 100, 200],\n",
    "            \"max_depth\": [None, 10, 20, 30],\n",
    "            \"min_samples_split\": [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    \"Support Vector Machine\": {\n",
    "        \"model\": SVC(random_state=42),\n",
    "        \"params\": {\n",
    "            \"C\": [0.01, 0.1, 1, 10],\n",
    "            \"kernel\": ['linear', 'rbf', 'poly'],\n",
    "            \"gamma\": ['scale', 'auto']\n",
    "        }\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"model\": GradientBoostingClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [50, 100, 200],\n",
    "            \"learning_rate\": [0.01, 0.1, 1]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# 2. Grid Search CV para cada modelo\n",
    "results = []\n",
    "confusion_matrices = []\n",
    "classification_reports = []\n",
    "\n",
    "for model_name, mp in model_params.items():\n",
    "    model = mp['model']\n",
    "    params = mp['params']\n",
    "    \n",
    "    print(f\"Starting Grid Search for: {model_name}\")\n",
    "    \n",
    "    # Definición del GridSearchCV\n",
    "    grid_search = GridSearchCV(model, params, cv=5, scoring='f1_weighted', verbose=1, n_jobs=-1)\n",
    "    \n",
    "    # Entrenamiento de GridSearchCV\n",
    "    grid_search.fit(X_train_tfidf, y_train_sampled)\n",
    "    \n",
    "    # Mejor combinación de hiperparámetros\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(f\"Best params for {model_name}: {grid_search.best_params_}\")\n",
    "    \n",
    "    print(f\"Infering with best model: {model_name}\")\n",
    "    # Predicción\n",
    "    y_pred = best_model.predict(X_test_tfidf)\n",
    "    \n",
    "    # Evaluación de métricas\n",
    "    accuracy = accuracy_score(y_test_sampled, y_pred)\n",
    "    precision = precision_score(y_test_sampled, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test_sampled, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test_sampled, y_pred, average='weighted')\n",
    "    \n",
    "    # Agregar resultados al DataFrame\n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Best Parameters\": grid_search.best_params_,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1\n",
    "    })\n",
    "\n",
    "    # Generar y guardar la matriz de confusión\n",
    "    cm = confusion_matrix(y_test_sampled, y_pred)\n",
    "    confusion_matrices.append({\"Model\": model_name, \"Confusion Matrix\": cm})\n",
    "    \n",
    "    # Generar y almacenar el classification report\n",
    "    report = classification_report(y_test_sampled, y_pred)\n",
    "    classification_reports.append({\"Model\": model_name, \"Classification Report\": report})\n",
    "\n",
    "# Convertir resultados a un DataFrame de pandas\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 3. Guardar resultados en un archivo CSV\n",
    "results_df.to_csv(os.path.join(output_dir, 'model_comparison_results_12000_CV.csv'), index=False)\n",
    "\n",
    "# Guardar la matriz de confusión como un archivo CSV\n",
    "confusion_matrices_df = pd.DataFrame(\n",
    "    [(item['Model'], item['Confusion Matrix'].tolist()) for item in confusion_matrices],\n",
    "    columns=['Model', 'Confusion Matrix']\n",
    ")\n",
    "confusion_matrices_df.to_csv(os.path.join(output_dir, 'confusion_matrices_12000_CV.csv'), index=False)\n",
    "\n",
    "# Guardar el classification report en un archivo de texto\n",
    "with open(os.path.join(output_dir, 'classification_reports_12000_CV.txt'), 'w') as file:\n",
    "    for report in classification_reports:\n",
    "        file.write(f\"Model: {report['Model']}\\n\")\n",
    "        file.write(f\"{report['Classification Report']}\\n\")\n",
    "        file.write(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# 4. Mostrar resultados\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "tfmvenv",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python (tfmvenv)",
   "language": "python",
   "name": "tfmvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
