{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55e0ff01-388a-4740-8c02-40c3004121d7",
   "metadata": {},
   "source": [
    "# Clasificación usando TF-IDF con algoritmos de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06975921-5f39-46a3-ae32-780e7d5cde8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "431159c9-6959-4c07-a696-845aab5af121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Leer el archivo Parquet en un DataFrame\n",
    "df = pd.read_parquet('./eda/mapped_&_usecases_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1182ce7-900d-4f72-9025-277753746785",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del DataFrame original: (115852, 7)\n",
      "Tamaño del DataFrame sampleado: (50000, 7)\n"
     ]
    }
   ],
   "source": [
    "# Crear una copia de 100,000 filas seleccionadas aleatoriamente\n",
    "df_sample = df.sample(n=50000, random_state=42).copy()\n",
    "\n",
    "# Verificar la forma del nuevo DataFrame\n",
    "print(f\"Tamaño del DataFrame original: {df.shape}\")\n",
    "print(f\"Tamaño del DataFrame sampleado: {df_sample.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "325cfefc-522b-4f78-93bf-7c719ce3c49f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of df1: (81096, 7)\n",
      "Size of df2: (34756, 7)\n",
      "Index(['source_code', 'slither_text', 'slither_label', 'use_cases',\n",
      "       'vulnerability_mapping', 'vulnerability_keys', 'vulnerable'],\n",
      "      dtype='object')\n",
      "Tamaño del conjunto de entrenamiento: (12164, 7)\n",
      "Tamaño del conjunto de validación: (12165, 7)\n",
      "Tamaño del conjunto de prueba: (10427, 7)\n",
      "Tamaño del conjunto de entrenamiento sampleado: (12000, 7)\n",
      "Tamaño del conjunto de validación sampleado: (5000, 7)\n",
      "Tamaño del conjunto de prueba sampleado: (500, 7)\n"
     ]
    }
   ],
   "source": [
    "# 3. Dividir los datos en conjunto de entrenamiento y prueba\n",
    "# Establecer una semilla para la reproducibilidad\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Dividir el DataFrame en dos partes iguales\n",
    "df1, df2 = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "# Verificar las dimensiones de las divisiones\n",
    "print(f\"Size of df1: {df1.shape}\")\n",
    "print(f\"Size of df2: {df2.shape}\")\n",
    "\n",
    "print(df2.columns)\n",
    "\n",
    "# Definir el tamaño de las particiones\n",
    "train_size = 0.7  # Porcentaje para el conjunto de entrenamiento\n",
    "val_size = 0.15   # Porcentaje para el conjunto de validación\n",
    "test_size = 0.15  # Porcentaje para el conjunto de prueba\n",
    "\n",
    "# Dividir el DataFrame en entrenamiento + validación y prueba\n",
    "train_val_df, test_df = train_test_split(df2, test_size=test_size + val_size, random_state=42)\n",
    "\n",
    "# Dividir el DataFrame de entrenamiento + validación en entrenamiento y validación\n",
    "val_size_adjusted = val_size / (val_size + test_size)  # Ajuste para la proporción en el conjunto de entrenamiento/validación\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=val_size_adjusted, random_state=42)\n",
    "               \n",
    "# Verificar tamaños de los conjuntos\n",
    "print(f\"Tamaño del conjunto de entrenamiento: {train_df.shape}\")\n",
    "print(f\"Tamaño del conjunto de validación: {val_df.shape}\")\n",
    "print(f\"Tamaño del conjunto de prueba: {test_df.shape}\")\n",
    "\n",
    "# Definir el número de muestras que deseas seleccionar\n",
    "num_train_samples = 12000\n",
    "num_val_samples = 5000\n",
    "num_test_samples = 500\n",
    "\n",
    "# Muestrear X muestras directamente desde los DataFrames\n",
    "train_df_sampled = train_df.sample(n=num_train_samples, replace=False, random_state=42)\n",
    "val_df_sampled = val_df.sample(n=num_val_samples, replace=False, random_state=42)\n",
    "test_df_sampled = test_df.sample(n=num_test_samples, replace=False, random_state=42)\n",
    "\n",
    "# Verificar tamaños de los conjuntos\n",
    "print(f\"Tamaño del conjunto de entrenamiento sampleado: {train_df_sampled.shape}\")\n",
    "print(f\"Tamaño del conjunto de validación sampleado: {val_df_sampled.shape}\")\n",
    "print(f\"Tamaño del conjunto de prueba sampleado: {test_df_sampled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1adde303-1341-4f62-a356-f6a1bb1238c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto y_train_sampled: (12000, 10)\n",
      "Tamaño del conjunto y_test_sampled: (500, 10)\n"
     ]
    }
   ],
   "source": [
    "# 1. Tokenización del código\n",
    "def tokenize_code(code):\n",
    "    # Expresión regular mejorada para capturar tokens específicos de Solidity\n",
    "    tokens = re.findall(r'\\b\\w+\\b|[{}()\\[\\];,=+\\-*/<>!&|%^~]', code)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Aplicar tokenización\n",
    "train_df_sampled['tokenized_code'] = train_df_sampled['source_code'].apply(tokenize_code)\n",
    "test_df_sampled['tokenized_code'] = test_df_sampled['source_code'].apply(tokenize_code)\n",
    "\n",
    "# 2. Preprocesar las etiquetas multiclase y multietiqueta\n",
    "# Dividir las etiquetas por ';' para convertirlas en una lista de etiquetas\n",
    "train_df_sampled['vulnerability_labels'] = train_df_sampled['vulnerability_mapping'].apply(lambda x: x.split(';'))\n",
    "test_df_sampled['vulnerability_labels'] = test_df_sampled['vulnerability_mapping'].apply(lambda x: x.split(';'))\n",
    "\n",
    "# Convertir las etiquetas de texto en un formato binarizado utilizando MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "df['vulnerability_labels'] = df['vulnerability_mapping'].apply(lambda x: x.split('; '))\n",
    "mlb.fit(df['vulnerability_labels'])\n",
    "\n",
    "# Transformar las etiquetas de los conjuntos de entrenamiento y prueba\n",
    "y_train_sampled = mlb.transform(train_df_sampled['vulnerability_labels'])\n",
    "y_test_sampled = mlb.transform(test_df_sampled['vulnerability_labels'])\n",
    "\n",
    "# Separar las características (X) y las etiquetas binarizadas (y)\n",
    "X_train_sampled = train_df_sampled['tokenized_code']\n",
    "X_test_sampled = test_df_sampled['tokenized_code']\n",
    "\n",
    "print(f\"Tamaño del conjunto y_train_sampled: {y_train_sampled.shape}\")\n",
    "print(f\"Tamaño del conjunto y_test_sampled: {y_test_sampled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78a6037a-eadd-4892-8e1e-010e366ccac2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4. Transformación TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_sampled)\n",
    "X_test_tfidf = tfidf.transform(X_test_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d71e4d-710c-43d5-a5d3-a1ae7fadb87c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto X_train_tfidf: (6000, 1000)\n",
      "Tamaño del conjunto X_test_tfidf: (500, 1000)\n",
      "Tamaño del conjunto y_train_sampled: (6000, 10)\n",
      "Tamaño del conjunto y_test_sampled: (500, 10)\n",
      "Training model: Logistic Regression\n",
      "Infering model: Logistic Regression\n",
      "Shape y_test_sampled: (500, 10)\n",
      "Shape y_pred: (500, 10)\n",
      "Training model: Random Forest\n",
      "Infering model: Random Forest\n",
      "Shape y_test_sampled: (500, 10)\n",
      "Shape y_pred: (500, 10)\n",
      "Training model: Support Vector Machine\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Crear el directorio de salida si no existe\n",
    "output_dir = './outputs/classification/tfidf/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 5. Definición de los modelos con OneVsRestClassifier\n",
    "models = {\n",
    "    \"Logistic Regression\": OneVsRestClassifier(LogisticRegression(random_state=42)),\n",
    "    \"Random Forest\": OneVsRestClassifier(RandomForestClassifier(random_state=42)),\n",
    "    \"Support Vector Machine\": OneVsRestClassifier(SVC(random_state=42)),\n",
    "    \"Gradient Boosting\": OneVsRestClassifier(GradientBoostingClassifier(random_state=42))\n",
    "}\n",
    "\n",
    "# 6. Entrenamiento, predicción y evaluación de cada modelo\n",
    "results = []\n",
    "confusion_matrices = []\n",
    "classification_reports = []\n",
    "\n",
    "print(f\"Tamaño del conjunto X_train_tfidf: {X_train_tfidf.shape}\")\n",
    "print(f\"Tamaño del conjunto X_test_tfidf: {X_test_tfidf.shape}\")\n",
    "print(f\"Tamaño del conjunto y_train_sampled: {y_train_sampled.shape}\")\n",
    "print(f\"Tamaño del conjunto y_test_sampled: {y_test_sampled.shape}\")\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training model: {model_name}\")\n",
    "    \n",
    "    # Entrenamiento\n",
    "    model.fit(X_train_tfidf, y_train_sampled)\n",
    "    \n",
    "    print(f\"Infering model: {model_name}\")\n",
    "    # Predicción\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    \n",
    "    print(f\"Shape y_test_sampled: {y_test_sampled.shape}\")\n",
    "    print(f\"Shape y_pred: {y_pred.shape}\")\n",
    "    \n",
    "    # Evaluación de métricas\n",
    "    accuracy = accuracy_score(y_test_sampled, y_pred)\n",
    "    precision = precision_score(y_test_sampled, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test_sampled, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test_sampled, y_pred, average='weighted')\n",
    "    \n",
    "    # Agregar resultados al DataFrame\n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1\n",
    "    })\n",
    "\n",
    "    # Generar y guardar la matriz de confusión (por cada etiqueta)\n",
    "    cm = confusion_matrix(y_test_sampled.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "    confusion_matrices.append({\"Model\": model_name, \"Confusion Matrix\": cm})\n",
    "    \n",
    "    # Generar y almacenar el classification report\n",
    "    report = classification_report(y_test_sampled, y_pred, target_names=mlb.classes_)\n",
    "    classification_reports.append({\"Model\": model_name, \"Classification Report\": report})\n",
    "\n",
    "# Convertir resultados a un DataFrame de pandas\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 7. Guardar resultados en un archivo CSV\n",
    "results_df.to_csv(os.path.join(output_dir, 'model_comparison_results_6000.csv'), index=False)\n",
    "\n",
    "# Guardar la matriz de confusión como un archivo CSV\n",
    "confusion_matrices_df = pd.DataFrame(\n",
    "    [(item['Model'], item['Confusion Matrix'].tolist()) for item in confusion_matrices],\n",
    "    columns=['Model', 'Confusion Matrix']\n",
    ")\n",
    "confusion_matrices_df.to_csv(os.path.join(output_dir, 'confusion_matrices_6000.csv'), index=False)\n",
    "\n",
    "# Guardar el classification report en un archivo de texto\n",
    "with open(os.path.join(output_dir, 'classification_reports_6000.txt'), 'w') as file:\n",
    "    for report in classification_reports:\n",
    "        file.write(f\"Model: {report['Model']}\\n\")\n",
    "        file.write(f\"{report['Classification Report']}\\n\")\n",
    "        file.write(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# 8. Mostrar resultados\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4728cc23-ef3c-405d-a48f-3770d4acf93a",
   "metadata": {},
   "source": [
    "Implementando Grid Search Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5dd9440-ca8a-469d-b101-a5f356f3dc4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto X_train_tfidf: (12000, 1000)\n",
      "Tamaño del conjunto X_test_tfidf: (500, 1000)\n",
      "Tamaño del conjunto y_train_sampled: (12000, 10)\n",
      "Tamaño del conjunto y_test_sampled: (500, 10)\n",
      "Tuning model: Logistic Regression\n",
      "Starting GridSearchCV for Logistic Regression...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best parameters for Logistic Regression: {'estimator__C': 10, 'estimator__solver': 'liblinear'}\n",
      "Predicting with model: Logistic Regression\n",
      "Tuning model: Random Forest\n",
      "Starting GridSearchCV for Random Forest...\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "[CV] END .....estimator__C=0.01, estimator__solver=liblinear; total time=   4.2s\n",
      "[CV] END .......estimator__C=10, estimator__solver=liblinear; total time=  11.5s\n",
      "[CV] END estimator__max_depth=10, estimator__n_estimators=100; total time= 2.4min\n",
      "[CV] END estimator__max_depth=20, estimator__n_estimators=50; total time= 2.0min\n",
      "[CV] END .....estimator__C=0.01, estimator__solver=liblinear; total time=   4.2s\n",
      "[CV] END .......estimator__C=10, estimator__solver=liblinear; total time=  11.8s\n",
      "[CV] END estimator__max_depth=10, estimator__n_estimators=200; total time= 4.7min\n",
      "[CV] END ......estimator__C=0.1, estimator__solver=liblinear; total time=   6.0s\n",
      "[CV] END estimator__max_depth=None, estimator__n_estimators=100; total time= 5.4min\n",
      "[CV] END ......estimator__C=0.1, estimator__solver=liblinear; total time=   6.0s\n",
      "[CV] END estimator__max_depth=None, estimator__n_estimators=100; total time= 5.5min\n",
      "[CV] END ............estimator__C=1, estimator__solver=lbfgs; total time=   5.6s\n",
      "[CV] END estimator__max_depth=None, estimator__n_estimators=100; total time= 5.5min\n",
      "[CV] END ..........estimator__C=0.1, estimator__solver=lbfgs; total time=   3.3s\n",
      "[CV] END ...........estimator__C=10, estimator__solver=lbfgs; total time=   7.3s\n",
      "[CV] END estimator__max_depth=10, estimator__n_estimators=50; total time= 1.2min\n",
      "[CV] END estimator__max_depth=10, estimator__n_estimators=200; total time= 4.5min\n",
      "[CV] END .........estimator__C=0.01, estimator__solver=lbfgs; total time=   2.1s\n",
      "[CV] END ........estimator__C=1, estimator__solver=liblinear; total time=   8.1s\n",
      "[CV] END estimator__max_depth=10, estimator__n_estimators=50; total time= 1.2min\n",
      "[CV] END estimator__max_depth=10, estimator__n_estimators=200; total time= 4.5min\n",
      "[CV] END ............estimator__C=1, estimator__solver=lbfgs; total time=   5.1s\n",
      "[CV] END estimator__max_depth=None, estimator__n_estimators=50; total time= 2.7min\n",
      "[CV] END estimator__max_depth=20, estimator__n_estimators=100; total time= 3.4min\n",
      "[CV] END .........estimator__C=0.01, estimator__solver=lbfgs; total time=   2.1s\n",
      "[CV] END ........estimator__C=1, estimator__solver=liblinear; total time=   9.1s\n",
      "[CV] END estimator__max_depth=10, estimator__n_estimators=100; total time= 2.4min\n",
      "[CV] END estimator__max_depth=20, estimator__n_estimators=100; total time= 3.8min\n",
      "[CV] END .....estimator__C=0.01, estimator__solver=liblinear; total time=   4.3s\n",
      "[CV] END estimator__max_depth=None, estimator__n_estimators=50; total time= 2.8min\n",
      "[CV] END estimator__max_depth=20, estimator__n_estimators=100; total time= 3.8min\n",
      "[CV] END ............estimator__C=1, estimator__solver=lbfgs; total time=   5.2s\n",
      "[CV] END estimator__max_depth=None, estimator__n_estimators=50; total time= 2.8min\n",
      "[CV] END estimator__max_depth=20, estimator__n_estimators=200; total time= 6.4min\n",
      "[CV] END .........estimator__C=0.01, estimator__solver=lbfgs; total time=   2.1s\n",
      "[CV] END ...........estimator__C=10, estimator__solver=lbfgs; total time=   7.8s\n",
      "[CV] END estimator__max_depth=None, estimator__n_estimators=200; total time= 9.4min\n",
      "[CV] END ......estimator__C=0.1, estimator__solver=liblinear; total time=   6.2s\n",
      "[CV] END estimator__max_depth=None, estimator__n_estimators=200; total time= 9.4min\n",
      "[CV] END ........estimator__C=1, estimator__solver=liblinear; total time=   9.7s\n",
      "[CV] END estimator__max_depth=None, estimator__n_estimators=200; total time= 9.6min\n",
      "[CV] END ..........estimator__C=0.1, estimator__solver=lbfgs; total time=   3.3s\n",
      "[CV] END ...........estimator__C=10, estimator__solver=lbfgs; total time=   7.3s\n",
      "[CV] END estimator__max_depth=10, estimator__n_estimators=50; total time= 1.2min\n",
      "[CV] END estimator__max_depth=20, estimator__n_estimators=50; total time= 2.0min\n",
      "[CV] END estimator__max_depth=20, estimator__n_estimators=200; total time= 6.5min\n",
      "[CV] END ..........estimator__C=0.1, estimator__solver=lbfgs; total time=   3.4s\n",
      "[CV] END .......estimator__C=10, estimator__solver=liblinear; total time=  11.9s\n",
      "[CV] END estimator__max_depth=10, estimator__n_estimators=100; total time= 2.4min\n",
      "[CV] END estimator__max_depth=20, estimator__n_estimators=50; total time= 2.0min\n",
      "[CV] END estimator__max_depth=20, estimator__n_estimators=200; total time= 6.0min\n",
      "Best parameters for Random Forest: {'estimator__max_depth': None, 'estimator__n_estimators': 200}\n",
      "Predicting with model: Random Forest\n",
      "Tuning model: Support Vector Machine\n",
      "Starting GridSearchCV for Support Vector Machine...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "[CV] END ..........estimator__C=10, estimator__kernel=linear; total time=11.1min\n",
      "[CV] END ...........estimator__C=1, estimator__kernel=linear; total time=11.4min\n",
      "[CV] END ...........estimator__C=1, estimator__kernel=linear; total time=11.5min\n",
      "[CV] END ...........estimator__C=1, estimator__kernel=linear; total time=11.5min\n",
      "[CV] END ..............estimator__C=1, estimator__kernel=rbf; total time=12.0min\n",
      "[CV] END ..............estimator__C=1, estimator__kernel=rbf; total time=12.0min\n",
      "[CV] END .............estimator__C=10, estimator__kernel=rbf; total time=12.2min\n",
      "[CV] END ..............estimator__C=1, estimator__kernel=rbf; total time=12.2min\n",
      "[CV] END .........estimator__C=0.1, estimator__kernel=linear; total time=12.2min\n",
      "[CV] END ............estimator__C=0.1, estimator__kernel=rbf; total time=12.4min\n",
      "[CV] END .........estimator__C=0.1, estimator__kernel=linear; total time=12.4min\n",
      "[CV] END .........estimator__C=0.1, estimator__kernel=linear; total time=12.5min\n",
      "[CV] END ............estimator__C=0.1, estimator__kernel=rbf; total time=12.5min\n",
      "[CV] END ............estimator__C=0.1, estimator__kernel=rbf; total time=12.5min\n",
      "[CV] END ..........estimator__C=10, estimator__kernel=linear; total time=10.9min\n",
      "[CV] END .............estimator__C=10, estimator__kernel=rbf; total time= 9.0min\n",
      "[CV] END ..........estimator__C=10, estimator__kernel=linear; total time=11.0min\n",
      "[CV] END .............estimator__C=10, estimator__kernel=rbf; total time= 9.3min\n",
      "Best parameters for Support Vector Machine: {'estimator__C': 10, 'estimator__kernel': 'rbf'}\n",
      "Predicting with model: Support Vector Machine\n",
      "Tuning model: Gradient Boosting\n",
      "Starting GridSearchCV for Gradient Boosting...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "[CV] END estimator__learning_rate=0.2, estimator__n_estimators=100; total time=27.8min\n",
      "[CV] END estimator__learning_rate=0.01, estimator__n_estimators=100; total time=27.8min\n",
      "[CV] END estimator__learning_rate=0.1, estimator__n_estimators=100; total time=27.9min\n",
      "[CV] END estimator__learning_rate=0.1, estimator__n_estimators=100; total time=27.9min\n",
      "[CV] END estimator__learning_rate=0.2, estimator__n_estimators=100; total time=27.9min\n",
      "[CV] END estimator__learning_rate=0.01, estimator__n_estimators=100; total time=28.0min\n",
      "[CV] END estimator__learning_rate=0.01, estimator__n_estimators=100; total time=28.0min\n",
      "[CV] END estimator__learning_rate=0.1, estimator__n_estimators=200; total time=46.3min\n",
      "[CV] END estimator__learning_rate=0.01, estimator__n_estimators=200; total time=47.9min\n",
      "[CV] END estimator__learning_rate=0.01, estimator__n_estimators=200; total time=48.0min\n",
      "[CV] END estimator__learning_rate=0.2, estimator__n_estimators=200; total time=48.0min\n",
      "[CV] END estimator__learning_rate=0.01, estimator__n_estimators=200; total time=48.2min\n",
      "[CV] END estimator__learning_rate=0.1, estimator__n_estimators=200; total time=52.7min\n",
      "[CV] END estimator__learning_rate=0.1, estimator__n_estimators=200; total time=52.8min\n",
      "[CV] END estimator__learning_rate=0.1, estimator__n_estimators=100; total time=27.5min\n",
      "[CV] END estimator__learning_rate=0.2, estimator__n_estimators=200; total time=38.0min\n",
      "[CV] END estimator__learning_rate=0.2, estimator__n_estimators=100; total time=26.8min\n",
      "[CV] END estimator__learning_rate=0.2, estimator__n_estimators=200; total time=39.3min\n",
      "Best parameters for Gradient Boosting: {'estimator__learning_rate': 0.2, 'estimator__n_estimators': 200}\n",
      "Predicting with model: Gradient Boosting\n",
      "                    Model                                    Best Parameters  \\\n",
      "0     Logistic Regression  {'estimator__C': 10, 'estimator__solver': 'lib...   \n",
      "1           Random Forest  {'estimator__max_depth': None, 'estimator__n_e...   \n",
      "2  Support Vector Machine   {'estimator__C': 10, 'estimator__kernel': 'rbf'}   \n",
      "3       Gradient Boosting  {'estimator__learning_rate': 0.2, 'estimator__...   \n",
      "\n",
      "   Accuracy  Precision  Recall  F1-Score  \n",
      "0     0.518   0.773242   0.532  0.619687  \n",
      "1     0.650   0.933803   0.650  0.760361  \n",
      "2     0.658   0.806810   0.662  0.722371  \n",
      "3     0.614   0.830595   0.644  0.720940  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Crear el directorio de salida si no existe\n",
    "output_dir = './outputs/classification/tfidf/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 5. Definición de los modelos con OneVsRestClassifier\n",
    "models = {\n",
    "    \"Logistic Regression\": OneVsRestClassifier(LogisticRegression(random_state=42)),\n",
    "    \"Random Forest\": OneVsRestClassifier(RandomForestClassifier(random_state=42)),\n",
    "    \"Support Vector Machine\": OneVsRestClassifier(SVC(random_state=42)),\n",
    "    \"Gradient Boosting\": OneVsRestClassifier(GradientBoostingClassifier(random_state=42))\n",
    "}\n",
    "\n",
    "# Definición de los hiperparámetros a probar con GridSearchCV para cada modelo\n",
    "param_grids = {\n",
    "    \"Logistic Regression\": {\n",
    "        'estimator__C': [0.01, 0.1, 1, 10],  # Regularización\n",
    "        'estimator__solver': ['lbfgs', 'liblinear']  # Solvers posibles\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        'estimator__n_estimators': [50, 100, 200],  # Número de árboles\n",
    "        'estimator__max_depth': [None, 10, 20]  # Profundidad máxima de los árboles\n",
    "    },\n",
    "    \"Support Vector Machine\": {\n",
    "        'estimator__C': [0.1, 1, 10],  # Regularización\n",
    "        'estimator__kernel': ['linear', 'rbf']  # Tipo de kernel\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        'estimator__n_estimators': [100, 200],  # Número de árboles\n",
    "        'estimator__learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "}\n",
    "\n",
    "# 6. Entrenamiento, predicción y evaluación de cada modelo con GridSearchCV\n",
    "results = []\n",
    "confusion_matrices = []\n",
    "classification_reports = []\n",
    "\n",
    "print(f\"Tamaño del conjunto X_train_tfidf: {X_train_tfidf.shape}\")\n",
    "print(f\"Tamaño del conjunto X_test_tfidf: {X_test_tfidf.shape}\")\n",
    "print(f\"Tamaño del conjunto y_train_sampled: {y_train_sampled.shape}\")\n",
    "print(f\"Tamaño del conjunto y_test_sampled: {y_test_sampled.shape}\")\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Tuning model: {model_name}\")\n",
    "    \n",
    "    # Configurar el GridSearchCV con verbosity para ver las combinaciones que se están probando\n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        param_grids[model_name], \n",
    "        scoring='f1_weighted', \n",
    "        cv=3, \n",
    "        n_jobs=-1, \n",
    "        verbose=2  # Nivel de verbose para imprimir el progreso de la búsqueda\n",
    "    )\n",
    "    \n",
    "    # Entrenamiento con validación cruzada\n",
    "    print(f\"Starting GridSearchCV for {model_name}...\")\n",
    "    grid_search.fit(X_train_tfidf, y_train_sampled)\n",
    "    \n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Obtener el mejor modelo\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"Predicting with model: {model_name}\")\n",
    "    # Predicción\n",
    "    y_pred = best_model.predict(X_test_tfidf)\n",
    "    \n",
    "    # Evaluación de métricas\n",
    "    accuracy = accuracy_score(y_test_sampled, y_pred)\n",
    "    precision = precision_score(y_test_sampled, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test_sampled, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test_sampled, y_pred, average='weighted')\n",
    "    \n",
    "    # Agregar resultados al DataFrame\n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Best Parameters\": grid_search.best_params_,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1\n",
    "    })\n",
    "\n",
    "    # Generar y guardar la matriz de confusión (por cada etiqueta)\n",
    "    cm = confusion_matrix(y_test_sampled.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "    confusion_matrices.append({\"Model\": model_name, \"Confusion Matrix\": cm})\n",
    "    \n",
    "    # Generar y almacenar el classification report\n",
    "    report = classification_report(y_test_sampled, y_pred, target_names=mlb.classes_)\n",
    "    classification_reports.append({\"Model\": model_name, \"Classification Report\": report})\n",
    "\n",
    "# Convertir resultados a un DataFrame de pandas\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 7. Guardar resultados en un archivo CSV\n",
    "results_df.to_csv(os.path.join(output_dir, 'model_comparison_results_12000_CV.csv'), index=False)\n",
    "\n",
    "# Guardar la matriz de confusión como un archivo CSV\n",
    "confusion_matrices_df = pd.DataFrame(\n",
    "    [(item['Model'], item['Confusion Matrix'].tolist()) for item in confusion_matrices],\n",
    "    columns=['Model', 'Confusion Matrix']\n",
    ")\n",
    "confusion_matrices_df.to_csv(os.path.join(output_dir, 'confusion_matrices_12000_CV.csv'), index=False)\n",
    "\n",
    "# Guardar el classification report en un archivo de texto\n",
    "with open(os.path.join(output_dir, 'classification_reports_12000_CV.txt'), 'w') as file:\n",
    "    for report in classification_reports:\n",
    "        file.write(f\"Model: {report['Model']}\\n\")\n",
    "        file.write(f\"{report['Classification Report']}\\n\")\n",
    "        file.write(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# 8. Mostrar resultados\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7066c23-2787-406e-ad55-59d7a5be6ba3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deteniendo la instancia 'tfm-final' en la ubicación 'europe-west1-b'...\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Configuración: nombre de la instancia y ubicación\n",
    "instance_name = 'tfm-final'  # Nombre de la instancia que deseas detener\n",
    "location = 'europe-west1-b'  # Zona de la instancia\n",
    "\n",
    "# Comando de gcloud para detener la instancia\n",
    "command = f\"gcloud workbench instances stop {instance_name} --location={location}\"\n",
    "\n",
    "try:\n",
    "    # Ejecutar el comando\n",
    "    print(f\"Deteniendo la instancia '{instance_name}' en la ubicación '{location}'...\")\n",
    "    result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "    # Mostrar salida\n",
    "    print(result.stdout.decode('utf-8'))\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error al detener la instancia: {e.stderr.decode('utf-8')}\")\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "tfmvenv",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python (tfmvenv)",
   "language": "python",
   "name": "tfmvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
