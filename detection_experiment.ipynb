{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46bc56a6-237d-4949-8510-583ad8a47a28",
   "metadata": {},
   "source": [
    "# Detección de vulnerabilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3f1a75e-1857-4ff7-bdd4-9404d5b0c4e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/tfmvenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/jupyter/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "token_hugg = \"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\"\n",
    "\n",
    "# Autenticarse en Hugging Face\n",
    "login(token=token_hugg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83a1bb8-8bba-42ca-9d86-df7ef2ff50a1",
   "metadata": {},
   "source": [
    "## Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543c14a2-d2cc-4e6c-9ef4-96416f84f176",
   "metadata": {},
   "source": [
    "### Carga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab409715-83c4-4923-b74a-c8aa3ce95b0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_parquet('./eda/mapped_&_usecases_df.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c53bdae-08e2-41a3-bd26-a276375b669b",
   "metadata": {},
   "source": [
    "### Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c175885-5709-4ee6-8275-4a67dd12644c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of df1: (81096, 2)\n",
      "Size of df2: (34756, 2)\n",
      "Index(['source_code', 'label'], dtype='object')\n",
      "Tamaño del conjunto de entrenamiento: 12164\n",
      "Tamaño del conjunto de validación: 12165\n",
      "Tamaño del conjunto de prueba: 10427\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Limpiar el dataset para usar únicamente las columnas source code y label\n",
    "df = data[['source_code', 'vulnerable']].copy()\n",
    "df = df.rename(columns={'vulnerable': 'label'})\n",
    "\n",
    "# Establecer una semilla para la reproducibilidad\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Dividir el DataFrame en dos partes iguales\n",
    "df1, df2 = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "# Verificar las dimensiones de las divisiones\n",
    "print(f\"Size of df1: {df1.shape}\")\n",
    "print(f\"Size of df2: {df2.shape}\")\n",
    "\n",
    "print(df2.columns)\n",
    "\n",
    "# Definir el tamaño de las particiones\n",
    "train_size = 0.7  # Porcentaje para el conjunto de entrenamiento\n",
    "val_size = 0.15   # Porcentaje para el conjunto de validación\n",
    "test_size = 0.15  # Porcentaje para el conjunto de prueba\n",
    "\n",
    "# Dividir el DataFrame en entrenamiento + validación y prueba\n",
    "train_val_df, test_df = train_test_split(df2, test_size=test_size + val_size, random_state=42)\n",
    "\n",
    "# Dividir el DataFrame de entrenamiento + validación en entrenamiento y validación\n",
    "val_size_adjusted = val_size / (val_size + test_size)  # Ajuste para la proporción en el conjunto de entrenamiento/validación\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=val_size_adjusted, random_state=42)\n",
    "\n",
    "# Convertir los DataFrames de nuevo a Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Verificar tamaños de los conjuntos\n",
    "print(f\"Tamaño del conjunto de entrenamiento: {len(train_dataset)}\")\n",
    "print(f\"Tamaño del conjunto de validación: {len(val_dataset)}\")\n",
    "print(f\"Tamaño del conjunto de prueba: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518a195a-f136-4896-adc0-30b8c4f415c4",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf24aaf-370a-4d7f-9dd1-bd19937c8fe8",
   "metadata": {},
   "source": [
    "### Definición del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "581aa0ff-c825-4f83-995f-891bb10dcf7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./outputs/detection/Salesforce/codet5-small\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Cargar el modelo\n",
    "#model_path = 'huggingface/CodeBERTa-small-v1'\n",
    "model_path = 'Salesforce/codet5-small'\n",
    "#model_path = 'codellama/CodeLlama-7b-hf'\n",
    "#model_path = 'meta-llama/CodeLlama-7b-hf'\n",
    "#model_path = 'google/codegemma-2b'\n",
    "\n",
    "entire_path = './outputs/detection/' + model_path\n",
    "print(entire_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc7c6c1-2b30-4d1a-baca-5083ae249e32",
   "metadata": {},
   "source": [
    "### Tokenizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cbf50ff-a092-4d5d-8ec3-c2ffa9346a1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/tfmvenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t5 tokenizer detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 12164/12164 [05:30<00:00, 36.78 examples/s]\n",
      "Map: 100%|██████████| 12165/12165 [05:25<00:00, 37.39 examples/s]\n",
      "Map: 100%|██████████| 10427/10427 [04:31<00:00, 38.37 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 13/13 [00:01<00:00,  7.17ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 13/13 [00:01<00:00,  7.30ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 11/11 [00:01<00:00,  6.97ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "322358974"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "max_input_length = 512\n",
    "max_target_length = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "if 'codet5' in model_path:\n",
    "    print('t5 tokenizer detected')\n",
    "    pad_on_right = tokenizer.padding_side == \"right\"\n",
    "elif 'CodeLlama' in model_path:\n",
    "    print('llama tokenizer detected')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "def preprocess_function(example):\n",
    "    # Si 'codet5' está en el nombre del modelo, usar la lógica específica para CodeT5\n",
    "    if 'codet5' in model_path:\n",
    "        # Tokenizar el código fuente para el modelo CodeT5\n",
    "        model_inputs = tokenizer(\n",
    "            example[\"source_code\"], \n",
    "            max_length=max_input_length, \n",
    "            truncation=True, \n",
    "            padding='max_length'\n",
    "        )\n",
    "        \n",
    "        # Asignar los labels directamente\n",
    "        model_inputs[\"labels\"] = example[\"label\"]\n",
    "    \n",
    "    # Si no es 'codet5', usar la lógica general\n",
    "    else:\n",
    "        model_inputs = tokenizer(\n",
    "            example[\"source_code\"], \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            max_length=512\n",
    "        )\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "# Aplicar la función de preprocesamiento al Dataset\n",
    "tok_dat_train = train_dataset.map(preprocess_function, batched=False)\n",
    "tok_dat_val = val_dataset.map(preprocess_function, batched=False)\n",
    "tok_dat_test = test_dataset.map(preprocess_function, batched=False)\n",
    "\n",
    "# Almacenar\n",
    "\n",
    "# Crear directorio si no existe\n",
    "os.makedirs(entire_path + '/data', exist_ok=True)\n",
    "\n",
    "# Guardar los datasets\n",
    "tok_dat_train.to_parquet(entire_path + '/data/tok_dat_train.parquet')\n",
    "tok_dat_val.to_parquet(entire_path + '/data/tok_dat_val.parquet')\n",
    "tok_dat_test.to_parquet(entire_path + '/data/tok_dat_test.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd658a7-0981-42a8-b93a-880be254f5ce",
   "metadata": {},
   "source": [
    "Ejecutar la celda de abajo en caso de que se quieran cargar los datasets para evitar el tiempo de tokenización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "392ecb06-30f6-4485-8651-64fe57744b44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/tfmvenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "if 'codet5' in model_path:\n",
    "    print('t5 tokenizer detected')\n",
    "    pad_on_right = tokenizer.padding_side == \"right\"\n",
    "elif 'CodeLlama' in model_path:\n",
    "    print('llama tokenizer detected')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Definir la ruta base donde se guardaron los archivos\n",
    "data_path = entire_path + '/data/'\n",
    "\n",
    "# Cargar los datasets desde archivos Parquet\n",
    "tok_dat_train = Dataset.from_parquet(data_path + 'tok_dat_train.parquet')\n",
    "tok_dat_val = Dataset.from_parquet(data_path + 'tok_dat_val.parquet')\n",
    "tok_dat_test = Dataset.from_parquet(data_path + 'tok_dat_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4115ac86-7a7b-45f4-b634-95d1e0a1167d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "\n",
    "# Muestrear X muestras directamente\n",
    "train_dataset_sampled = tok_dat_train.select(np.random.choice(len(tok_dat_train), 12000, replace=False))\n",
    "val_dataset_sampled = tok_dat_val.select(np.random.choice(len(tok_dat_val), 5000, replace=False))\n",
    "test_dataset_sampled = tok_dat_test.select(np.random.choice(len(tok_dat_test), 500, replace=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251db77e-dfde-42d4-9c3d-869837b9a060",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Función evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebcfe9a-d2ff-4271-bb45-e10f7f844471",
   "metadata": {},
   "source": [
    "Este compute funciona correctamente para CodeBERTa, pero no para CodeT5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f68ee915-67f1-48f0-82d1-bf87fc2c7ac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655a38ee-9d75-4907-9d99-3950adbd6b41",
   "metadata": {},
   "source": [
    "Este compute funciona correctamente para CodeT5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f25b60c-249e-4caa-9517-48ac38065fec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Si predictions es un tuple, toma el primer elemento\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    \n",
    "    # Toma el argmax a lo largo del último eje para obtener las predicciones finales\n",
    "    preds = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    # Asegúrate de que labels y preds tienen la misma forma\n",
    "    labels = np.array(labels).flatten()\n",
    "    preds = np.array(preds).flatten()\n",
    "    \n",
    "    # Calcula la métrica\n",
    "    accuracy = metric.compute(predictions=preds, references=labels)\n",
    "    \n",
    "    # Imprime el resultado para ver las claves disponibles\n",
    "    print(f\"Resultados de la métrica: {accuracy}\")\n",
    "    \n",
    "    # Devuelve todo el resultado para inspeccionarlo\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a246bc2-24ed-46d9-9ecd-86b8b15c40ba",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d35a3b-040d-4460-995e-8463e6545a95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at Salesforce/codet5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/jupyter/tfmvenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/jupyter/tfmvenv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [188/188 10:45:16, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.563708</td>\n",
       "      <td>0.724800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.524713</td>\n",
       "      <td>0.746000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.517064</td>\n",
       "      <td>0.756200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/40 12:41 < 01:52, 0.04 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de la métrica: {'accuracy': 0.7248}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/jupyter/tfmvenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/jupyter/tfmvenv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de la métrica: {'accuracy': 0.746}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/jupyter/tfmvenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/jupyter/tfmvenv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de la métrica: {'accuracy': 0.7562}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/jupyter/tfmvenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/jupyter/tfmvenv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding, TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = entire_path + \"/training_12000samples/\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=5,\n",
    "    report_to='none',\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_accumulation_steps=2,  # Acumula gradientes durante 2 pasos\n",
    "    fp16=True,  # Habilita entrenamiento en precisión mixta\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_num_workers=4,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=train_dataset_sampled,\n",
    "   eval_dataset=val_dataset_sampled,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17bf79a-9b3e-4893-880f-d5649a14fd28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Guardar modelo\n",
    "\n",
    "model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(entire_path + '/trained_model_12000samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba7e7a7-3d41-4e13-8d67-660647b4f275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Configuración: nombre de la instancia y ubicación\n",
    "instance_name = 'tfm-final'  # Nombre de la instancia que deseas detener\n",
    "location = 'europe-west1-b'  # Zona de la instancia\n",
    "\n",
    "# Comando de gcloud para detener la instancia\n",
    "command = f\"gcloud workbench instances stop {instance_name} --location={location}\"\n",
    "\n",
    "try:\n",
    "    # Ejecutar el comando\n",
    "    print(f\"Deteniendo la instancia '{instance_name}' en la ubicación '{location}'...\")\n",
    "    result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "    # Mostrar salida\n",
    "    print(result.stdout.decode('utf-8'))\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error al detener la instancia: {e.stderr.decode('utf-8')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdeaf75-4eae-40bb-8499-4593996c7eac",
   "metadata": {},
   "source": [
    "### Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "980bdb52-d589-4729-b0f2-35091d2b980b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cargar el modelo entrenado\n",
    "tuned_model = AutoModelForSequenceClassification.from_pretrained(entire_path + '/trained_model_12000samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "793244ea-1498-4caa-b2f1-6741a431ec74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at huggingface/CodeBERTa-small-v1 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding, TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "\n",
    "if 'CodeLlama' in model_path:\n",
    "    print('llama tokenizer detected')\n",
    "    \n",
    "    # Asegurarse de que el modelo reconoce el token de padding\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Redimensionar las embeddings del modelo si se ha añadido un token\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Predecir\n",
    "\n",
    "test_args = TrainingArguments(\n",
    "    output_dir = entire_path + '/inference_12000samples/',\n",
    "    do_train = False,\n",
    "    do_predict = True,\n",
    "    per_device_eval_batch_size = 128,\n",
    "    dataloader_drop_last = False,\n",
    "    save_total_limit=1,\n",
    "    dataloader_num_workers=4\n",
    ")\n",
    "\n",
    "# init trainer\n",
    "tester = Trainer(\n",
    "              #model = model,\n",
    "              model = tuned_model,\n",
    "              data_collator=data_collator,\n",
    "              args = test_args,\n",
    "              compute_metrics=compute_metrics,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51fc54a2-1f34-4586-9d48-f2b398965ecb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_results = tester.predict(test_dataset_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30b2c281-1e14-4088-b1a8-f288d1ed25c9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in test_results.predictions: 2\n",
      "Element 0: Type = <class 'numpy.ndarray'>\n",
      "  Shape = (500, 10)\n",
      "Element 1: Type = <class 'numpy.ndarray'>\n",
      "  Shape = (500, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "# Imprimir el número de elementos en la tupla\n",
    "print(f\"Number of elements in test_results.predictions: {len(test_results.predictions)}\")\n",
    "\n",
    "# Imprimir el tipo y tamaño de cada elemento en la tupla\n",
    "for i, element in enumerate(test_results.predictions):\n",
    "    print(f\"Element {i}: Type = {type(element)}\")\n",
    "\n",
    "    # Si el elemento es un tensor de PyTorch o array de NumPy, imprime su forma\n",
    "    if isinstance(element, (torch.Tensor, np.ndarray)):\n",
    "        print(f\"  Shape = {element.shape}\")\n",
    "    \n",
    "    # Si el elemento es una lista, imprime su longitud y la longitud de los subelementos si existen\n",
    "    elif isinstance(element, list):\n",
    "        print(f\"  Length = {len(element)}\")\n",
    "        if len(element) > 0 and isinstance(element[0], list):\n",
    "            print(f\"  Length of first subelement = {len(element[0])}\")\n",
    "    else:\n",
    "        print(f\"  Value = {element}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71522d6c-0545-4eeb-8896-d1b0de1f8756",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/tfmvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_code</th>\n",
       "      <th>label</th>\n",
       "      <th>label_pred</th>\n",
       "      <th>label_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1254351</th>\n",
       "      <td>contract ERC20Basic {\\n  function totalSupply(...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.818256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072718</th>\n",
       "      <td>// SPDX-License-Identifier: Unlicensed        ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.755442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133551</th>\n",
       "      <td>/**\\n * \\n * \\n * ELON TWEETED ABOUT US\\n  \\n ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.027349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540270</th>\n",
       "      <td>/**\\n *Submitted for verification at Etherscan...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.979263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441815</th>\n",
       "      <td>/**\\n   \\nApe 1\\n\\n🌗TOKYO GHOUL ERC-20 🌗\\n\\nTo...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.963246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               source_code  label  label_pred  \\\n",
       "1254351  contract ERC20Basic {\\n  function totalSupply(...      0           1   \n",
       "1072718  // SPDX-License-Identifier: Unlicensed        ...      1           1   \n",
       "1133551  /**\\n * \\n * \\n * ELON TWEETED ABOUT US\\n  \\n ...      1           0   \n",
       "540270   /**\\n *Submitted for verification at Etherscan...      1           1   \n",
       "1441815  /**\\n   \\nApe 1\\n\\n🌗TOKYO GHOUL ERC-20 🌗\\n\\nTo...      1           1   \n",
       "\n",
       "         label_prob  \n",
       "1254351    0.818256  \n",
       "1072718    0.755442  \n",
       "1133551    0.027349  \n",
       "540270     0.979263  \n",
       "1441815    0.963246  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "\n",
    "# Samplear el dataframe de test\n",
    "# Obtener los índices de las filas aleatorias\n",
    "sample_indices = np.random.choice(test_df.index, 500, replace=False)\n",
    "\n",
    "# Muestrear el DataFrame\n",
    "test_df_sampled = test_df.loc[sample_indices]\n",
    "\n",
    "test_df_sampled['label_pred'] = test_results[0].argmax(1) # Use this to visually check\n",
    "test_df_sampled['label_prob'] = 1-nn.Softmax()(torch.tensor(test_results[0]))[:,0] #You should push this one\n",
    "\n",
    "# Crear directorio si no existe\n",
    "os.makedirs(entire_path + '/results', exist_ok=True)\n",
    "test_df_sampled.to_csv(entire_path + '/results/inf_model_12000.csv', index=False)\n",
    "\n",
    "test_df_sampled.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0415ac1c-3a80-4c40-93c7-09ae23df97e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Análisis de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52d6a98d-f811-44bc-a8d5-f575144c4d58",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6260\n",
      "Precision: 0.7282\n",
      "Recall: 0.7667\n",
      "F1 Score: 0.7470\n",
      "Confusion Matrix:\n",
      "[[ 37 103]\n",
      " [ 84 276]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.26      0.28       140\n",
      "           1       0.73      0.77      0.75       360\n",
      "\n",
      "    accuracy                           0.63       500\n",
      "   macro avg       0.52      0.52      0.52       500\n",
      "weighted avg       0.61      0.63      0.62       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "y_true = test_df_sampled['label']\n",
    "y_pred = test_df_sampled['label_pred']\n",
    "\n",
    "# Precisión (Accuracy)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "# Precisión (Precision)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "# Sensibilidad (Recall)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "# Puntaje F1 (F1 Score)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "# Matriz de confusión (Confusion Matrix)\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "# Reporte de clasificación (Classification Report)\n",
    "class_report = classification_report(y_true, y_pred)\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3f9bf8-265f-47e2-a49b-7b236f3663fb",
   "metadata": {},
   "source": [
    "#### Métricas globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7142e0cc-c05c-41c3-a120-6e302cfe8b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, matthews_corrcoef, log_loss, brier_score_loss, balanced_accuracy_score\n",
    "\n",
    "import os\n",
    "\n",
    "# Extraer la probabilidad de la clase positiva de 'label_prob'\n",
    "y_prob = test_df_sampled['label_prob']  # Esto asume que label_prob es una lista de listas con probabilidades\n",
    "\n",
    "# Predicciones de clase\n",
    "y_pred = test_df_sampled['label_pred']  # Etiquetas predichas\n",
    "\n",
    "# Cálculo de métricas adicionales\n",
    "roc_auc = roc_auc_score(y_true, y_prob)  # AUC-ROC\n",
    "mcc = matthews_corrcoef(y_true, y_pred)  # Matthews Correlation Coefficient\n",
    "log_loss_value = log_loss(y_true, y_prob)  # Log Loss\n",
    "brier_score = brier_score_loss(y_true, y_prob)  # Brier Score\n",
    "balanced_accuracy = balanced_accuracy_score(y_true, y_pred)  # Balanced Accuracy\n",
    "\n",
    "# Confusion matrix para calcular Specificity\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "specificity = tn / (tn + fp)  # Specificity\n",
    "\n",
    "# Crear un DataFrame con todas las métricas\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC', 'MCC', 'Log Loss', 'Brier Score', 'Specificity', 'Balanced Accuracy'],\n",
    "    'Value': [accuracy, precision, recall, f1, roc_auc, mcc, log_loss_value, brier_score, specificity, balanced_accuracy]\n",
    "})\n",
    "\n",
    "# Crear el directorio de salida si no existe\n",
    "os.makedirs(entire_path + '/results', exist_ok=True)\n",
    "\n",
    "# Guardar las métricas en un archivo CSV\n",
    "metrics_df.to_csv(entire_path + '/results/metrics_summary_12000samples.csv', index=False)\n",
    "\n",
    "# Guardar la matriz de confusión en un archivo CSV\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, columns=['Predicted Negative', 'Predicted Positive'], index=['Actual Negative', 'Actual Positive'])\n",
    "conf_matrix_df.to_csv(entire_path + '/results/confusion_matrix_12000.csv', index=True)\n",
    "\n",
    "# Guardar el classification report en un archivo de texto\n",
    "with open(entire_path + '/results/classification_report_12000.txt', 'w') as f:\n",
    "    f.write(f\"Classification Report:\\n{class_report}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12c0348-4921-416d-9f47-72958dd41bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "tfmvenv",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python (tfmvenv)",
   "language": "python",
   "name": "tfmvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
